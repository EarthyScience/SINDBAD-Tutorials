{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e70e68e9",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4b9bad96",
   "metadata": {},
   "source": [
    "## üìì Hybrid Inversion with SINDBAD-Tutorials\n",
    "A notebook by Sujan Koirala, Xu Shan, Jialiang Zhou and Nuno Carvalhais\n",
    "\n",
    "---\n",
    "\n",
    "### üìå Purpose\n",
    "\n",
    "This notebook sets up and runs hybrid inversion experiments using the SINDBAD-Tutorials framework. It includes training of ML-based observation operators and the evaluation of model parameters through data assimilation.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7888f58",
   "metadata": {},
   "source": [
    "## SINDBAD\n",
    "[SINDBAD](http://sindbad-mdi.org/) is a model-data integration framework for terrestrial carbon-water processes [[Koirala et al., in prep.](https://essopenarchive.org/users/551954/articles/1271244)]. Is built in Julia with a view on speed and differenciability for the development of representation of processes and responses of ecosystem functioning to meteorological conditions and changes in climate. Sets on the concept of modularity to formaly test hypothesis on the representation of processes / models ($f(X,\\theta)$), for given observational constraints ($Y$) and drivers ($X$) of the carbon and water dynamics in terrestrial ecosystems. Modularity is extended to the initial condition problem ($\\text{x}^*_0$), cost functions ($\\mathcal{L(\\theta)}$) and optimization algorithms ($\\mathcal{O}$). SINDBAD integrates machine learning for enhancing the representation of processes in mechanistically-inspired models, hybrid modeling [Reichstein et al., 2019], by learning ML-based parameterizations [e.g. Bao et al., 2024], paving way for process abstraction [Son et al., 2024].\n",
    "\n",
    "---\n",
    "## WROASTED: a Simple Coupled Carbon‚ÄìWater Ecosystem Model\n",
    "The carbon dynamics,  $\\frac{dC}{dt}$, are simulated as the difference between gross assimilation and respiratory fluxes\n",
    "$$\n",
    "\\frac{dC}{dt} = GPP - R_{ECO}\n",
    "$$\n",
    "\n",
    "where ${GPP}$, gross primary productivity, results from photosynthetic activity and $R_{ECO}$, ecosystem respiration, is the sum of autotrophic and heterotrophic respiratory fluxes, namely, $R_{A}$ and $R_{H}$. \n",
    "\n",
    "$R_{A}$ integrates both maintenance and growth respiration, $R_{M}$ and $R_{G}$, where  $R_{M}$  can be generically written like:\n",
    "\n",
    "$$\n",
    "R_{M} =\\sum_{i=1}^{N} \\tau_i \\cdot C_i \\cdot f_T \n",
    "$$\n",
    "\n",
    "$i$ representing the different carbon pools ($C_i$) in vegetation - root/wood/leaf/reserves; $\\tau_i$ the turnover rate of pool $i$ , and $f_T$ the temperature dependence of metabolic activity, usually a $Q_{10}$ function; while $R_G=Y_G \\cdot GPP$, being $Y_G$ and constant growth efficiency parameter [see Amthor, 2001]. \n",
    "\n",
    "$R_H$ results from litter and soil decomposition:\n",
    "$$\n",
    "R_{H} =\\sum_{i=1}^{N} \\tau_i \\cdot C_i \\cdot f_T \\cdot f_W\n",
    "$$\n",
    "\n",
    "$i$ representing the different heterotrophic carbon pools ($C_i$) in soils - fast and slow litter and organic carbon pools; $\\tau_i$ the turnover rate of pool $i$ , $f_T$ and $f_W$ the temperature and soil moisture sensitivity of decomposition function.\n",
    "\n",
    "Soil moisture dynamics, $\\frac{dW}{dt}$:\n",
    "$$\n",
    "\\frac{dW}{dt} = P_r - E_i - E_s - Q - D - T_r\n",
    "$$\n",
    "\n",
    "Being: $Pr$: precipitation; $E_i$: interception evaporation; $E_s$: soil evaporation; $Q$: surface runoff; $D$: drainage; $T_r$: plant transpiration.\n",
    "\n",
    "Transpiration is tighly coupled to $GPP$, estimated as: \n",
    "\n",
    "$$\n",
    "GPP = min(GPP_D,GPP_S)\n",
    "$$\n",
    "\n",
    "Being demand $GPP$:\n",
    "$$\n",
    "GPP_S = \\epsilon^* \\cdot f\\text{APAR} \\cdot \\text{PAR} \\cdot (f_L \\cdot f_{CI} \\cdot f_T \\cdot f_{VPD} \\cdot f_W)\n",
    "$$\n",
    "The product between: maximum light use efficiency, $\\epsilon^*$; the fraction of photosynthetically active radiation, $\\text{APAR}$, absorbed by leafs, $f\\text{APAR}$; and the instantaneous effect of light intensity $f_L$, cloudiness index $f_CI$, vapor pressure deficit $f_VPD$ and soil moisture $f_W$ [see Bao et al., 2023; 2024].\n",
    "\n",
    "And  supply $GPP$:\n",
    "\n",
    "$$\n",
    "GPP_S = PAW^{k_{Tr}} \\cdot WUE\n",
    "$$\n",
    "\n",
    "Where where the daily variations in water use efficiency, $WUE$, result from changes in $VPD$ and $\\quad [CO_2]_{atm}$. Upon $C$ assimilation by vegetation, and deduced $R_A$ costs, the available carbon is transported to the different vegetation pools depending on environmental conditions, as inspired by the growind season index (GSI) model [see Koirala et al., in print; Jolly et al., 2005]. \n",
    "\n",
    "Overall, WROASTED includes >40 parameters controlling the responses of carbon and water dynamics in terrestrial ecosystems constrainable by observations of ecosystem fluxes, eddy covariance, plant phenology from remote sensing EO data, and above ground biomass stocks, where available [see Koirala et al., in print].\n",
    "\n",
    "---\n",
    "\n",
    "## Simple LUE-model\n",
    "Now we set a simpler model, to further test the hybrid modeling setup where solely:\n",
    "$$\n",
    "GPP = \\epsilon^* \\cdot f\\text{APAR} \\cdot \\text{PAR} \\cdot (f_L \\cdot f_{CI} \\cdot f_T \\cdot f_{VPD} \\cdot f_W)\n",
    "$$\n",
    "\n",
    "where there is not supply limitation of GPP,  $f_L=f_W=1$, $f_{VPD}$ follows PRELES [REF], $f_T$ follows CASA [Potter et al., 1993], $f_{CI}$ [Wang et al., 2015] . $f\\text{APAR}$ is a constant.\n",
    "\n",
    "---\n",
    "## The challenge\n",
    "To calibrate and generalize the model parameterization in space, across sites. \n",
    "\n",
    "In parameter inversion, the goal is to find $\\theta$ such that the model predictions $f(X, \\theta)$ best match observed datasets $y$. Here, the terrestrial ecosystem model, WROASTED, represented by $f(X, \\theta)$, predicts a set of ecosystem carbon and water state and flux variables, $\\hat{y}$, observed at locations, where: $X$: meteorological drivers (i.e., temperature, radiation, precipitation, $VPD$, etc); $\\theta$: parameter vector to be estimated; $y$: observations (e.g., $GPP$, $T_r$, evapotranspiration, $R_{ECO}$, aboveground biomass AGB, $f\\text{APAR}$). This can generically can be written:\n",
    "$$\n",
    "\\theta^*=\\arg\\min_{\\theta \\in \\Theta} \\; \\mathcal{L}(\\theta)\\quad\\text{via}\\quad\\mathcal{O}\n",
    "$$\n",
    "\n",
    "Where: $\\mathcal{L}(\\theta)$: is the cost function quantifying the mismatch between model predictions and observations; $\\Theta$: feasible parameter space (e.g., bounds or priors on $\\theta$); $\\mathcal{O}$: optimization operator/algorithm (e.g., gradient descent, L-BFGS, CMA-ES).\n",
    "\n",
    "Now, for generalizing $\\theta$, the challenge is to solve the following composite problem:\n",
    "$$\n",
    "h(X)=(f‚àòg)(X)=f(X,g(\\dot{X}))\n",
    "$$\n",
    "Where, like above, $f(X,\\theta)$ is a terrestrial ecosystem model, and $g(\\dot{X})$ models the parameters $\\theta$ of $f(X,\\theta)$ depending on a ser of spatially varying features, $\\dot{X}$, charaterizing conditions at locations where $y$ is observed.\n",
    "\n",
    "As in the previous inversion exercise, for fluxes and phenology time series, the loss function $\\mathcal{L}(\\theta)$ is set to the normalized Nash-Sutcliffe Efficiency (NNSE)\n",
    "$$\n",
    "\\text{NNSE}(\\theta) = 1 - \\frac{1}{2-NSE}\n",
    "$$\n",
    "$$\n",
    "\\text{NSE}(\\theta) = 1 - \\frac{\\sum_{i=1}^{N} (y_i - f(X_i, \\theta))^2}{\\sum_{i=1}^{N} (y_i - \\bar{y})^2}\n",
    "$$\n",
    "\n",
    "While for stocks, AGB, an adjusted normalized mean average error is used\n",
    "$$\n",
    "NMAE = \\frac{\\sum_{i=1}^{N} |y_i - f(X_i, \\theta)|}{N \\cdot (1+ \\bar{y})}\n",
    "$$\n",
    "$$\n",
    "\\theta^* = \\arg\\min_{\\theta} \\; \\mathcal{L}(\\theta)\n",
    "$$\n",
    "\n",
    "---\n",
    "## Setting up SINDBAD-Tutorials\n",
    "Navigate to the [SINDBAD-Tutorials for AI4PEX repository](GitHubLink) and install. Please follow instructions. For us, [VS Code](https://code.visualstudio.com/) has been a very fluid host for [Julia](https://julialang.org/) developments.\n",
    "\n",
    "### Get the data for these SINDBAD tutorials\n",
    "The data can be found [here](https://nextcloud.bgc-jena.mpg.de/s/w2mbH59W4nF3Tcd). Suggestion, store it in a child folder of the SINDBAD-Tutorials (e.g. SINDBAD-Tutorials/data/)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a8d22f5",
   "metadata": {},
   "source": [
    "### ‚öôÔ∏è Setup & Dependencies\n",
    "\n",
    "- Uses Julia packages:\n",
    "  - `SindbadTutorials`, `SindbadML`, `Revise`\n",
    "  - `Plots` for visualization\n",
    "- Custom helper functions are imported via `tutorial_helpers.jl`\n",
    "- User and OS setup is handled for Windows compatibility\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd5d4509",
   "metadata": {},
   "outputs": [],
   "source": [
    "import Pkg\n",
    "Pkg.activate(\".\")\n",
    "Pkg.instantiate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39231d4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ================================== using tools ==================================================\n",
    "# some of the things that will be using... Julia tools, SINDBAD tools, local codes...\n",
    "using Revise\n",
    "using SindbadTutorials\n",
    "using SindbadML\n",
    "using SindbadML.Random\n",
    "using SindbadTutorials.Plots\n",
    "\n",
    "include(\"tutorial_helpers.jl\")\n",
    "\n",
    "## get the sites to run experiment on\n",
    "selected_site_indices = getSiteIndicesForHybrid();\n",
    "do_random = 0# set to integer values larger than zero to use random selection of #do_random sites\n",
    "if do_random > 0\n",
    "    Random.seed!(1234)\n",
    "    selected_site_indices = first(shuffle(selected_site_indices), do_random)\n",
    "end\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5ba60ff",
   "metadata": {},
   "source": [
    "### Data and Paths\n",
    "Data to be used can be found here: [Nextcloud Link](https://nextcloud.bgc-jena.mpg.de/s/w2mbH59W4nF3Tcd)  \n",
    "Organizing the paths of data sources and outputs for this experiment.  \n",
    "Paths include:\n",
    "- `path_input`: Zarr data for site-level runs\n",
    "- `path_observation`: Observation data (in the same file)\n",
    "- `path_covariates`: Covariates data used in ML training\n",
    "- `path_output`: Directory for experiment outputs\n",
    "\n",
    "Two configurations are shown:\n",
    "1. **Initial Setup**: A simple experiment (commented out to avoid long run time).\n",
    "2. **WROASTED Setup**: Uses a more complex model setup and runs the full pipeline including:\n",
    "   - Loading experiment settings JSON\n",
    "   - Preparing the hybrid inversion environment\n",
    "   - Training the machine learning model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63f1cd92",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ================================== get data / set paths ========================================= \n",
    "# data to be used can be found here: https://nextcloud.bgc-jena.mpg.de/s/w2mbH59W4nF3Tcd\n",
    "# organizing the paths of data sources and outputs for this experiment\n",
    "path_input_dir      = getSindbadDataDepot(; env_data_depot_var=\"SINDBAD_DATA_DEPOT\", \n",
    "                    local_data_depot=joinpath(\"/home/jovyan/data/ellis_jena_2025\")); # for convenience, the data file is set within the SINDBAD-Tutorials path; this needs to be changed otherwise.\n",
    "path_input          = joinpath(\"$(path_input_dir)\",\"FLUXNET_v2023_12_1D_REPLACED_Noise003_v1.zarr\"); # zarr data source containing all the data for site level runs\n",
    "path_observation    = path_input; # observations (synthetic or otherwise) are included in the same file\n",
    "path_covariates     = joinpath(\"$(path_input_dir)\",\"CovariatesFLUXNET.zarr\"); # zarr data source containing all the covariates\n",
    "path_output         = \"\";"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4f50b41",
   "metadata": {},
   "source": [
    "This next cell sets the experiment to run WROASTED. It takes a while. Is left here for experimenting with time, or a bigger machine..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b9ded52",
   "metadata": {},
   "outputs": [],
   "source": [
    "#= this one takes a hugh amount of time, leave it here for reference\n",
    "# ================================== setting up the experiment ====================================\n",
    "# experiment is all set up according to a (collection of) json file(s)\n",
    "path_experiment_json    = joinpath(@__DIR__,\"..\",\"ellis_jena_2025\",\"settings_WROASTED_HB\",\"experiment_hybrid.json\");\n",
    "path_training_folds     = \"\";#joinpath(@__DIR__,\"..\",\"ellis_jena_2025\",\"settings_WROASTED_HB\",\"nfolds_sites_indices.jld2\");\n",
    "\n",
    "replace_info = Dict(\n",
    "    \"forcing.default_forcing.data_path\" => path_input,\n",
    "    \"forcing.subset.site\" => selected_site_indices,\n",
    "    \"optimization.observations.default_observation.data_path\" => path_observation,\n",
    "    \"optimization.optimization_cost_threaded\" => false,\n",
    "    \"optimization.optimization_parameter_scaling\" => nothing,\n",
    "    \"hybrid.ml_training.fold_path\" => nothing,\n",
    "    \"hybrid.covariates.path\" => path_covariates,\n",
    ");\n",
    "\n",
    "# generate the info and other helpers\n",
    "info            = getExperimentInfo(path_experiment_json; replace_info=deepcopy(replace_info));\n",
    "forcing         = getForcing(info);\n",
    "observations    = getObservation(info, forcing.helpers);\n",
    "sites_forcing   = forcing.data[1].site;\n",
    "hybrid_helpers  = prepHybrid(forcing, observations, info, info.hybrid.ml_training.method);\n",
    "\n",
    "# ================================== train the hybrid model =======================================\n",
    "trainML(hybrid_helpers, info.hybrid.ml_training.method)\n",
    "=#"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3efe6e4d",
   "metadata": {},
   "source": [
    "Continue the setup of the LUE model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d98f69e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ================================== change setup to LUE ==========================================\n",
    "# same as before, but for a faster / simpler LUE model\n",
    "path_experiment_json    = joinpath(@__DIR__,\"..\",\"ellis_jena_2025\",\"settings_LUE\",\"experiment_hybrid.json\");\n",
    "path_training_folds     = \"\";#joinpath(@__DIR__,\"..\",\"ellis_jena_2025\",\"settings_WROASTED_HB\",\"nfolds_sites_indices.jld2\");\n",
    "\n",
    "replace_info = Dict(\n",
    "    \"forcing.default_forcing.data_path\" => path_input,\n",
    "    \"forcing.subset.site\" => selected_site_indices,\n",
    "    \"optimization.observations.default_observation.data_path\" => path_observation,\n",
    "    \"optimization.optimization_cost_threaded\" => false,\n",
    "    \"optimization.optimization_parameter_scaling\" => nothing,\n",
    "    \"hybrid.ml_training.fold_path\" => nothing,\n",
    "    \"hybrid.covariates.path\" => path_covariates,\n",
    ");"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b17e1529",
   "metadata": {},
   "source": [
    "### generate the info and other helpers\n",
    "To execute the experiment, helper structures are generated using the JSON settings and associated data paths. These include:\n",
    "- `info`: the core configuration object (experiment.json, optimization.json, forcing.json) for the experiment\n",
    "- `forcing`: configurations about meteorological forcing data\n",
    "- `observations`: synthetic or real measurements of fluxes and states of the terrestrial biosphere model in SINDBAD\n",
    "- `hybrid_helpers`: encapsulates feature preparation, training, and prediction functions...etc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "903027b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate the info and other helpers\n",
    "info            = getExperimentInfo(path_experiment_json; replace_info=deepcopy(replace_info));\n",
    "forcing         = getForcing(info);\n",
    "observations    = getObservation(info, forcing.helpers);\n",
    "sites_forcing   = forcing.data[1].site;\n",
    "hybrid_helpers  = prepHybrid(forcing, observations, info, info.hybrid.ml_training.method);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a6d60c5",
   "metadata": {},
   "source": [
    "### Train the model\n",
    "The training of the machine learning model is performed using the `trainML` function:\n",
    "```julia\n",
    "trainML(hybrid_helpers, info.hybrid.ml_training.method)\n",
    "```\n",
    "This step uses the specified training method and data prepared in the previous steps to fit a parameter estimation model.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3dbe85e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train the model\n",
    "trainML(hybrid_helpers, info.hybrid.ml_training.method)\n",
    "## check the docs for output at: http://sindbad-mdi.org/pages/develop/hybrid_modeling.html and http://sindbad-mdi.org/pages/develop/sindbad_outputs.html\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e573a40",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b1cd3d33",
   "metadata": {},
   "source": [
    "### Posterior Diagnostics\n",
    "After training, the model is used to infer site-level parameters from the input features:\n",
    "```julia\n",
    "params_sites = ml_model(xfeatures)\n",
    "```\n",
    "These inferred parameters are then rescaled to their actual physical ranges using:\n",
    "```julia\n",
    "scaled_params_sites = getParamsAct(params_sites, info.optimization.parameter_table)\n",
    "```\n",
    "This step provides a site-specific view of parameter values, which can be used for diagnostic evaluation or fed into the forward model for simulation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7c92fce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# retrieve the inversed parameters\n",
    "sites_forcing = forcing.data[1].site;\n",
    "ml_model = hybrid_helpers.ml_model;\n",
    "xfeatures = hybrid_helpers.features.data;\n",
    "loss_functions = hybrid_helpers.loss_functions;\n",
    "loss_component_functions = hybrid_helpers.loss_component_functions;\n",
    "\n",
    "params_sites = ml_model(xfeatures)\n",
    "@info \"params_sites: [$(minimum(params_sites)), $(maximum(params_sites))]\"\n",
    "\n",
    "scaled_params_sites = getParamsAct(params_sites, info.optimization.parameter_table)\n",
    "@info \"scaled_params_sites: [$(minimum(scaled_params_sites)), $(maximum(scaled_params_sites))]\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb70eefa",
   "metadata": {},
   "source": [
    "### Site-level Evaluation\n",
    "To investigate performance at a specific site, one can select a site by index and extract the corresponding inferred parameters. The selected parameters are passed to the site-specific loss function to compute the loss value and its components:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26cc2879",
   "metadata": {},
   "outputs": [],
   "source": [
    "# select a site\n",
    "# idx = findfirst(x -> x == \"DE-Hai\", sites_forcing)\n",
    "site_index = 67\n",
    "site_name = sites_forcing[site_index]\n",
    "\n",
    "loc_params = scaled_params_sites(site=site_name).data.data\n",
    "loss_f_site = loss_functions(site=site_name);\n",
    "loss_vector_f_site = loss_component_functions(site=site_name);\n",
    "@time loss_f_site(loc_params)\n",
    "loss_vector_f_site(loc_params)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2a3a742",
   "metadata": {},
   "source": [
    "### Site Simulation with Custom Parameters\n",
    "The following function is used to run the terrestrial ecosystem model (TEM) for a selected site using either default or optimized parameters:\n",
    "The function returns the simulated outputs, the loss vector, and the total scalar loss. Example usages include the following codes.\n",
    "These calls allow for a direct comparison between the default and optimized model runs at the site level.\n",
    "\n",
    "#### Arguments:\n",
    "- `selected_models`: A tuple of all models selected in the given model structure.\n",
    "- `loc_forcing`: A forcing NamedTuple containing the time series of environmental drivers for a single location.\n",
    "- `loc_spinup_forcing`: A forcing NamedTuple for spinup, used to initialize the model to a steady state (only used if spinup is enabled).\n",
    "- `loc_forcing_t`: A forcing NamedTuple for a single location and a single time step.\n",
    "- `loc_output`: An output array or view for storing the model outputs for a single location.\n",
    "- `loc_land`: Initial SINDBAD land NamedTuple with all fields and subfields.\n",
    "- `tem_info`: A helper NamedTuple containing necessary objects for model execution and type consistencies.\n",
    "\n",
    "The function returns the simulated outputs, the loss vector, and the total scalar loss.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20cb648a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# run the model for the site with the default parameters\n",
    "function run_model_param(info, forcing, observations, site_index, loc_params)\n",
    "    # info = @set info.helpers.run.land_output_type = PreAllocArrayAll();\n",
    "    run_helpers = prepTEM(info.models.forward, forcing, observations, info);\n",
    "    # output all variables\n",
    "    # info = @set info.output.variables = run_helpers.output_vars;\n",
    "\n",
    "    params = loc_params;\n",
    "    selected_models = info.models.forward;\n",
    "    parameter_scaling_type = info.optimization.run_options.parameter_scaling;\n",
    "    tbl_params = info.optimization.parameter_table;\n",
    "    param_to_index = getParameterIndices(selected_models, tbl_params);\n",
    "\n",
    "    models = updateModels(params, param_to_index, parameter_scaling_type, selected_models);\n",
    "\n",
    "    loc_forcing = run_helpers.space_forcing[site_index];\n",
    "    loc_spinup_forcing = run_helpers.space_spinup_forcing[site_index];\n",
    "    loc_forcing_t = run_helpers.loc_forcing_t;\n",
    "    loc_output = getCacheFromOutput(run_helpers.space_output[site_index], info.hybrid.ml_gradient.method);\n",
    "    gradient_lib = info.hybrid.ml_gradient.method;\n",
    "    loc_output_from_cache = getOutputFromCache(loc_output, params, gradient_lib);\n",
    "    land_init = deepcopy(run_helpers.loc_land);\n",
    "    tem_info = run_helpers.tem_info;\n",
    "    loc_obs = run_helpers.space_observation[site_index];\n",
    "    loc_cost_option = prepCostOptions(loc_obs, info.optimization.cost_options);\n",
    "    constraint_method = info.optimization.run_options.multi_constraint_method;\n",
    "    coreTEM!(\n",
    "            models,\n",
    "            loc_forcing,\n",
    "            loc_spinup_forcing,\n",
    "            loc_forcing_t,\n",
    "            loc_output_from_cache,\n",
    "            land_init,\n",
    "            tem_info);\n",
    "    forward_output = (; Pair.(getUniqueVarNames(info.output.variables), loc_output_from_cache)...)\n",
    "    loss_vector = SindbadTutorials.metricVector(loc_output_from_cache, loc_obs, loc_cost_option);\n",
    "    t_loss = combineMetric(loss_vector, constraint_method);\n",
    "    return forward_output, loss_vector, t_loss\n",
    "end\n",
    "output_default_site, _, _ = run_model_param(info, forcing, observations, \n",
    "                                            site_index, \n",
    "                                            info.optimization.parameter_table.default);\n",
    "\n",
    "\n",
    "\n",
    "# run the model for the site with the parameters from the hybrid \n",
    "output_optimized_site, _, _ = run_model_param(info, forcing, observations, \n",
    "                                                site_index, \n",
    "                                                loc_params);\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf21df82",
   "metadata": {},
   "source": [
    "### Visualization and Summary Statistics\n",
    "After running the model with both default and optimized parameters, this section compares their performances using time series plots and statistical metrics such as NSE (Nash‚ÄìSutcliffe Efficiency). The results are visualized for each site:\n",
    "- Time series of observed vs. modeled outputs for each variable\n",
    "- Overlay of default and optimized model outputs\n",
    "- Evaluation metric (e.g., NSE) shown in the plot legend\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08dae5c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# do plots, compute some simple statistics e.g. NSE\n",
    "def_dat = output_default_site;\n",
    "opt_dat = output_optimized_site;\n",
    "loc_observation = [Array(o[:, site_index]) for o in observations.data];\n",
    "costOpt = prepCostOptions(loc_observation, info.optimization.cost_options);\n",
    "default(titlefont=(20, \"times\"), legendfontsize=18, tickfont=(15, :blue), )\n",
    "foreach(costOpt) do var_row\n",
    "    v = var_row.variable\n",
    "    println(\"plot obs::\", v)\n",
    "    v = (var_row.mod_field, var_row.mod_subfield)\n",
    "    vinfo = getVariableInfo(v, info.experiment.basics.temporal_resolution)\n",
    "    v = vinfo[\"standard_name\"]\n",
    "    lossMetric = var_row.cost_metric\n",
    "    loss_name = nameof(typeof(lossMetric))\n",
    "    if loss_name in (:NNSEInv, :NSEInv)\n",
    "        lossMetric = NSE()\n",
    "    end\n",
    "    (obs_var, obs_œÉ, def_var) = getData(def_dat, loc_observation, var_row)\n",
    "    (_, _, opt_var) = getData(opt_dat, loc_observation, var_row)\n",
    "    obs_var_TMP = obs_var[:, 1, 1, 1]\n",
    "    non_nan_index = findall(x -> !isnan(x), obs_var_TMP)\n",
    "    if length(non_nan_index) < 2\n",
    "        tspan = 1:length(obs_var_TMP)\n",
    "    else\n",
    "        tspan = first(non_nan_index):last(non_nan_index)\n",
    "    end\n",
    "    obs_œÉ = obs_œÉ[tspan]\n",
    "    obs_var = obs_var[tspan, 1, 1, 1]\n",
    "    def_var = def_var[tspan, 1, 1, 1]\n",
    "    opt_var = opt_var[tspan, 1, 1, 1]\n",
    "\n",
    "    xdata = [info.helpers.dates.range[tspan]...]\n",
    "    obs_var_n, obs_œÉ_n, def_var_n = getDataWithoutNaN(obs_var, obs_œÉ, def_var)\n",
    "    obs_var_n, obs_œÉ_n, opt_var_n = getDataWithoutNaN(obs_var, obs_œÉ, opt_var)\n",
    "    metr_def = metric(obs_var_n, obs_œÉ_n, def_var_n, lossMetric)\n",
    "    metr_opt = metric(obs_var_n, obs_œÉ_n, opt_var_n, lossMetric)\n",
    "    p1 = plot(xdata, obs_var; label=\"obs\", seriestype=:scatter, mc=:black, ms=4, lw=0, ma=0.65, left_margin=1Plots.cm)\n",
    "    plot!(xdata, def_var, color=:steelblue2, lw=1.5, ls=:dash, left_margin=1Plots.cm, legend=:outerbottom, \n",
    "        legendcolumns=3, label=\"def ($(round(metr_def, digits=2)))\", size=(800, 400), \n",
    "        title=\"$(vinfo[\"long_name\"]) ($(vinfo[\"units\"])) -> $(nameof(typeof(lossMetric)))\")\n",
    "    plot!(xdata, opt_var; color=:seagreen3, label=\"opt ($(round(metr_opt, digits=2)))\", lw=1.5, ls=:dash)\n",
    "    display(p1)\n",
    "    savefig(joinpath(info.output.dirs.figure, \"wroasted_$(site_name)_$(v).png\"))\n",
    "end\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbfa8152",
   "metadata": {},
   "source": [
    "Additionally, for all sites, the scalar loss values (e.g., inverse NSE) are computed and stored. These are then visualized using a histogram to assess the distribution of performance across the full site ensemble:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c97646a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# preallocate\n",
    "n_sites = length(sites_forcing);\n",
    "t_loss_def = Vector{Float32}(undef, n_sites);\n",
    "t_loss_opt = Vector{Float32}(undef, n_sites);\n",
    "\n",
    "# for (i, site_index) in enumerate(1:n_sites)\n",
    "# i = 1\n",
    "# for site_index in hybrid_helpers.indices.testing\n",
    "for (i, site_index) in enumerate(1:n_sites)\n",
    "    @info site_index\n",
    "    @info sites_forcing[site_index]\n",
    "    @info \"running using default parameters...\"\n",
    "    # # the following way could be faster to run...\n",
    "    # loss_f_site = loss_functions(site=site_name);\n",
    "    # loss_vector_f_site = loss_component_functions(site=site_name);\n",
    "    # loss_f_site(loc_params)\n",
    "    # loss_vector_f_site(loc_params)\n",
    "\n",
    "    # run default\n",
    "    _, lv_def, tl_def = run_model_param(\n",
    "       info, forcing, observations, site_index,\n",
    "       info.optimization.parameter_table.default\n",
    "    )\n",
    "    # run optimized\n",
    "    @info \"running using optimized parameters...\"\n",
    "    _, lv_opt, tl_opt = run_model_param(\n",
    "       info, forcing, observations, site_index,\n",
    "       scaled_params_sites(site=sites_forcing[site_index]).data.data\n",
    "    )\n",
    "    # collect\n",
    "    t_loss_def[i]     = tl_def\n",
    "    t_loss_opt[i]     = tl_opt\n",
    "    # i += 1\n",
    "end\n",
    "\n",
    "# now plot histogram of scalar losses\n",
    "h1 = histogram(\n",
    "  t_loss_def,\n",
    "  bins = 50,\n",
    "  alpha = 0.5,\n",
    "  label = \"default\",\n",
    "  xlabel = \"NSE\",\n",
    "  ylabel = \"Count\",\n",
    "  title = \"NSE: default vs optimized\"\n",
    ")\n",
    "histogram!(\n",
    "  t_loss_opt,\n",
    "  bins = 50,\n",
    "  alpha = 0.5,\n",
    "  label = \"optimized\"\n",
    ")\n",
    "display(h1)\n",
    "savefig(joinpath(info.output.dirs.figure, \"loss_histogram_scalar.png\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1726ed89",
   "metadata": {},
   "outputs": [],
   "source": [
    "# now plot histogram of scalar losses\n",
    "h2 = histogram(\n",
    "  t_loss_def .- t_loss_opt,\n",
    "  bins = 50,\n",
    "  alpha = 0.5,\n",
    "  label = \"default\",\n",
    "  xlabel = \"NSE\",\n",
    "  ylabel = \"Count\",\n",
    "  title = \"NSE: default - optimized\"\n",
    ")\n",
    "# histogram!(\n",
    "#   t_loss_opt,\n",
    "#   bins = 50,\n",
    "#   alpha = 0.5,\n",
    "#   label = \"optimized\"\n",
    "# )\n",
    "display(h2)\n",
    "savefig(joinpath(info.output.dirs.figure, \"loss_diff_histogram_scalar.png\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "613e77a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "hybrid_helpers"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.11.5",
   "language": "julia",
   "name": "julia-1.11"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
